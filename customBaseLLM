import base64
import requests
from typing import Any, Dict, List, Optional
from langchain_core.language_models import BaseLanguageModel
from langchain_core.outputs import Generation, LLMResult
from langchain_core.tools import BaseTool
from pydantic import BaseModel, Field

class CustomLLMConfig(BaseModel):
    endpoint: str = Field(..., description="API endpoint for the LLM")
    username: str = Field(..., description="Username for basic authentication")
    password: str = Field(..., description="Password for basic authentication")

class CustomLLM(BaseLanguageModel):
    config: CustomLLMConfig
    tools: List[Any] = []

    def __init__(self, endpoint: str, username: str, password: str):
        super().__init__(config=CustomLLMConfig(endpoint=endpoint, username=username, password=password))
        self.tools = []

    def _call(self, prompt: str, stop: Optional[List[str]] = None, **kwargs) -> str:
        """
        Synchronous call to the LLM API.
        """
        headers = self._get_auth_headers()
        payload = {
            "prompt": prompt,
            "tools": self._get_tool_schemas() if self.tools else None
        }
        if stop:
            payload["stop"] = stop

        try:
            response = requests.post(self.config.endpoint, json=payload, headers=headers)
            response.raise_for_status()
            return response.json().get("text", "")
        except requests.RequestException as e:
            raise ValueError(f"API call failed: {str(e)}")

    def _generate(self, prompts: List[str], stop: Optional[List[str]] = None, **kwargs) -> LLMResult:
        """
        Generate responses for multiple prompts.
        """
        generations = []
        for prompt in prompts:
            text = self._call(prompt, stop=stop, **kwargs)
            generations.append([Generation(text=text)])
        return LLMResult(generations=generations)

    async def _agenerate(self, prompts: List[str], stop: Optional[List[str]] = None, **kwargs) -> LLMResult:
        """
        Asynchronous generation (simulated here using synchronous call for simplicity).
        """
        return self._generate(prompts, stop=stop, **kwargs)

    def bind_tools(self, tools: List[Any], **kwargs) -> 'CustomLLM':
        """
        Bind tools to the LLM, enabling tool usage in agent workflows.
        Args:
            tools: List of tools (BaseTool instances or function schemas).
            **kwargs: Additional arguments for tool binding.
        Returns:
            Self for method chaining.
        """
        self.tools = tools
        return self

    def _get_auth_headers(self) -> Dict[str, str]:
        """
        Generate basic authentication headers.
        """
        credentials = f"{self.config.username}:{self.config.password}"
        auth_header = f"Basic {base64.b64encode(credentials.encode()).decode()}"
        return {
            "Authorization": auth_header,
            "Content-Type": "application/json"
        }

    def _get_tool_schemas(self) -> List[Dict[str, Any]]:
        """
        Convert bound tools to API-compatible schemas.
        """
        schemas = []
        for tool in self.tools:
            if isinstance(tool, BaseTool):
                schema = {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.args_schema.schema() if tool.args_schema else {}
                }
            elif isinstance(tool, dict):
                schema = tool  # Assume dict is already in correct schema format
            else:
                raise ValueError(f"Unsupported tool type: {type(tool)}")
            schemas.append(schema)
        return schemas

    @property
    def _llm_type(self) -> str:
        """
        Identify the LLM type for LangChain.
        """
        return "custom_llm"
