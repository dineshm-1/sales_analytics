import os
import json
import requests # Required for making HTTP requests
from typing import Any, List, Optional, Sequence, Union, Dict, Iterator, Type

# Langchain core imports
from langchain_core.callbacks import CallbackManagerForLLMRun
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import (
    AIMessage,
    AIMessageChunk,
    BaseMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
)
from langchain_core.outputs import ChatGeneration, ChatResult, ChatGenerationChunk
from langchain_core.pydantic_v1 import BaseModel, Field, SecretStr, root_validator
from langchain_core.tools import BaseTool
from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env

# --- Custom Chat Model Implementation ---

class CustomChatModelWithAuth(BaseChatModel):
    """
    Custom LangChain Chat Model Wrapper for an LLM API requiring basic authentication.

    Assumes the API:
    - Accepts POST requests at `api_url`.
    - Expects JSON body with 'messages' and optionally 'tools'.
    - Authenticates via a header (e.g., 'Authorization: Bearer <key>' or 'X-API-Key: <key>').
    - Returns JSON with 'content' and optionally 'tool_calls'.
    """

    # Configuration fields
    api_url: str = Field(..., description="The endpoint URL of the custom LLM API.")
    api_key: SecretStr = Field(..., description="The API key for authentication.")
    auth_header: str = Field("Authorization", description="The HTTP header name for authentication.")
    auth_prefix: str = Field("Bearer ", description="The prefix for the auth header value (e.g., 'Bearer '). Leave empty if none.")
    model_name: str = Field("custom-model", description="Identifier for the custom model.")
    temperature: float = Field(0.7, description="Sampling temperature for the LLM.")
    # Add other LLM parameters as needed (e.g., top_p, max_tokens)

    # Pydantic validator to get API key from environment if not provided directly
    @root_validator(pre=True)
    def validate_environment(cls, values: Dict[str, Any]) -> Dict[str, Any]:
        """Validate that api key and endpoint exists in environment or is passed explicitly."""
        values["api_key"] = convert_to_secret_str(
            get_from_dict_or_env(values, "api_key", "CUSTOM_LLM_API_KEY") # Looks for CUSTOM_LLM_API_KEY env var
        )
        values["api_url"] = get_from_dict_or_env(
            values, "api_url", "CUSTOM_LLM_API_URL", # Looks for CUSTOM_LLM_API_URL env var
        )
        return values

    @property
    def _llm_type(self) -> str:
        """Return type of chat model."""
        return "custom-chat-model-with-auth"

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """Get the identifying parameters."""
        return {
            "model_name": self.model_name,
            "api_url": self.api_url,
            "auth_header": self.auth_header,
            "auth_prefix": self.auth_prefix,
            "temperature": self.temperature,
            # Add other parameters here
        }

    def _format_messages(self, messages: List[BaseMessage]) -> List[Dict[str, Any]]:
        """Formats LangChain messages into the structure expected by the custom API."""
        formatted_messages = []
        for msg in messages:
            role = ""
            content = ""
            tool_calls_api = None # Placeholder for tool calls in API format

            if isinstance(msg, HumanMessage):
                role = "user"
                content = msg.content
            elif isinstance(msg, AIMessage):
                role = "assistant"
                content = msg.content
                # If the AI message itself contains tool calls (from previous turns)
                # and your API expects them back, format them here.
                # This part depends heavily on your specific API requirements.
                if msg.tool_calls:
                    # Example: Format back for API (adjust as needed)
                    # tool_calls_api = [
                    #     {"id": tc["id"], "function": {"name": tc["name"], "arguments": json.dumps(tc["args"])}}
                    #     for tc in msg.tool_calls
                    # ]
                    pass # Add formatting logic if needed
            elif isinstance(msg, SystemMessage):
                role = "system"
                content = msg.content
            elif isinstance(msg, ToolMessage):
                role = "tool" # Or 'function' depending on API
                content = msg.content # The result of the tool call
                # Your API might expect tool_call_id here as well
                # formatted_messages.append({"role": role, "tool_call_id": msg.tool_call_id, "content": content})
                # continue # Skip default append if using custom format above
            else:
                raise ValueError(f"Unsupported message type: {type(msg)}")

            msg_dict = {"role": role, "content": content}
            # Add tool calls if formatted above and API expects them
            # if tool_calls_api:
            #     msg_dict["tool_calls"] = tool_calls_api
            formatted_messages.append(msg_dict)

        print(f"Formatted Messages for API: {formatted_messages}") # Debug print
        return formatted_messages

    def _format_tools(self, tools: Sequence[Union[Dict, Type[BaseModel], BaseTool]]) -> Optional[List[Dict]]:
        """Formats LangChain tools into the structure expected by the custom API."""
        if not tools:
            return None

        formatted_tools = []
        # This assumes an OpenAI-like tool format. Adjust to your API's specific schema.
        for tool_spec in tools:
            if isinstance(tool_spec, BaseTool):
                formatted_tools.append(
                    {
                        "type": "function",
                        "function": {
                            "name": tool_spec.name,
                            "description": tool_spec.description,
                            "parameters": tool_spec.args_schema.schema() if tool_spec.args_schema else {},
                        }
                    }
                )
            elif isinstance(tool_spec, dict) and "type" in tool_spec and "function" in tool_spec:
                 # Assume it's already in the correct format
                 formatted_tools.append(tool_spec)
            elif isinstance(tool_spec, type) and issubclass(tool_spec, BaseModel):
                 # Pydantic model definition
                 schema = tool_spec.schema()
                 formatted_tools.append(
                    {
                        "type": "function",
                        "function": {
                            "name": schema.get("title", tool_spec.__name__),
                            "description": schema.get("description", ""),
                            "parameters": schema,
                        }
                    }
                 )
            else:
                raise ValueError(f"Unsupported tool specification type: {type(tool_spec)}")

        print(f"Formatted Tools for API: {formatted_tools}") # Debug print
        return formatted_tools


    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None, # Note: Handling 'stop' depends on API support
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """
        Main method to interact with the custom LLM API.
        Handles message formatting, tool binding, authentication, and response parsing.
        """
        print(f"--- Custom LLM _generate called ---")
        print(f"kwargs received: {kwargs}")

        # 1. Format Messages
        formatted_messages = self._format_messages(messages)

        # 2. Format Tools (Check kwargs from bind_tools first, then self.tools)
        #    `bind_tools` passes tools via kwargs['tools']
        bound_tools = kwargs.get("tools") or getattr(self, "tools", None)
        formatted_tools = self._format_tools(bound_tools) if bound_tools else None

        # 3. Prepare Request Payload
        payload = {
            "messages": formatted_messages,
            "model": self.model_name, # Include if your API requires it
            "temperature": self.temperature,
            # Add other parameters your API supports (e.g., max_tokens, top_p)
            # "max_tokens": 1024,
        }
        if formatted_tools:
            payload["tools"] = formatted_tools
            # Your API might have a specific parameter for enabling tools
            # payload["tool_choice"] = "auto" # Example

        # Add stop sequences if your API supports them
        if stop:
            payload["stop"] = stop

        print(f"API Payload: {json.dumps(payload, indent=2)}") # Debug print

        # 4. Prepare Headers for Authentication
        headers = {
            "Content-Type": "application/json",
            self.auth_header: f"{self.auth_prefix}{self.api_key.get_secret_value()}"
        }
        print(f"API Headers: {{'Content-Type': 'application/json', '{self.auth_header}': '{self.auth_prefix}***REDACTED***'}}") # Debug print (hide key)

        # 5. Make API Call
        try:
            response = requests.post(
                self.api_url,
                headers=headers,
                json=payload,
                timeout=60 # Add a timeout
            )
            response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)
            response_json = response.json()
            print(f"API Raw Response: {json.dumps(response_json, indent=2)}") # Debug print

        except requests.exceptions.RequestException as e:
            raise ConnectionError(f"Error calling Custom LLM API at {self.api_url}: {e}") from e
        except json.JSONDecodeError as e:
             raise ValueError(f"Error decoding JSON response from Custom LLM API: {e}. Response text: {response.text}") from e


        # 6. Parse Response
        content = response_json.get("content", "") # Adjust key based on your API response
        ai_message = AIMessage(content=content or "") # Content can be empty if only tool calls

        # --- Tool Call Parsing ---
        # This assumes your API returns tool calls in an OpenAI-compatible format.
        # Adjust the parsing logic based on your API's actual response structure.
        raw_tool_calls = response_json.get("tool_calls")
        if raw_tool_calls and isinstance(raw_tool_calls, list):
            parsed_tool_calls = []
            invalid_tool_calls = []
            for call in raw_tool_calls:
                try:
                    # Example structure: {"id": "call_abc", "function": {"name": "tool_name", "arguments": '{"arg1": "val1"}'}}
                    func_info = call.get("function", {})
                    tool_name = func_info.get("name")
                    tool_args_raw = func_info.get("arguments")
                    tool_call_id = call.get("id")

                    if not tool_name or tool_args_raw is None or tool_call_id is None:
                        raise ValueError("Missing required fields (name, arguments, id) in tool call")

                    # Parse JSON arguments string into a dict
                    tool_args = json.loads(tool_args_raw)

                    parsed_tool_calls.append(
                        {
                            "name": tool_name,
                            "args": tool_args,
                            "id": tool_call_id,
                        }
                    )
                except json.JSONDecodeError as e:
                    # Handle cases where the LLM hallucinates invalid JSON arguments
                    print(f"Warning: Could not parse tool arguments JSON: {e}")
                    invalid_tool_calls.append(
                        {
                            "name": tool_name,
                            "args": tool_args_raw, # Store raw args on error
                            "id": tool_call_id,
                            "error": f"JSONDecodeError: {e}",
                        }
                    )
                except Exception as e:
                     print(f"Warning: Error processing tool call: {e}")
                     invalid_tool_calls.append(
                        {
                            "name": call.get("function", {}).get("name", "unknown"),
                            "args": call.get("function", {}).get("arguments", ""),
                            "id": call.get("id", "unknown"),
                            "error": f"ProcessingError: {e}",
                        }
                     )

            if parsed_tool_calls:
                ai_message.tool_calls = parsed_tool_calls
                # Store raw calls for potential debugging if needed
                # ai_message.additional_kwargs["raw_tool_calls"] = raw_tool_calls
            if invalid_tool_calls:
                 ai_message.invalid_tool_calls = invalid_tool_calls

        # --- End Tool Call Parsing ---

        # Add token usage or other metadata if provided by your API
        # usage_metadata = response_json.get("usage")
        # generation_info = {"finish_reason": response_json.get("finish_reason")}

        generation = ChatGeneration(
            message=ai_message,
            # generation_info=generation_info,
            # usage_metadata=usage_metadata # LangChain expects specific structure if used
        )
        return ChatResult(generations=[generation])

    def _stream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[ChatGenerationChunk]:
        """Streaming implementation (basic placeholder)."""
        # Streaming requires your API to support Server-Sent Events (SSE) or similar.
        # Implementing streaming correctly, especially with tool calls, is complex.
        # This basic version just calls _generate and yields a single chunk.
        # For true streaming, you'd use `requests.post(..., stream=True)`
        # and iterate over `response.iter_lines()` or `response.iter_content()`,
        # parsing each chunk and yielding AIMessageChunks.

        # --- Placeholder Implementation ---
        # try:
        #     result = self._generate(messages, stop=stop, run_manager=run_manager, **kwargs)
        #     generation = result.generations[0]
        #     yield ChatGenerationChunk(message=AIMessageChunk(
        #         content=generation.message.content,
        #         tool_call_chunks=generation.message.tool_calls # Simplified - real streaming needs chunking logic
        #         # Add other fields like usage_metadata if available
        #     ))
        # except Exception as e:
        #      # Handle errors during generation if needed for streaming context
        #      yield ChatGenerationChunk(message=AIMessageChunk(content=f"Error: {e}"))
        # --- End Placeholder ---

        # Raise error indicating streaming is not fully implemented
        raise NotImplementedError("Streaming is not fully implemented for this custom model.")


# --- Example Usage ---
if __name__ == "__main__":
    from dotenv import load_dotenv
    from langchain_core.tools import tool

    # Load environment variables (optional, if storing API key/URL in .env)
    load_dotenv()

    # --- Define a simple tool ---
    @tool
    def get_weather(location: str) -> str:
        """Gets the current weather for a specified location."""
        print(f"--- TOOL CALLED: get_weather(location='{location}') ---")
        # In a real scenario, this would call a weather API
        if "boston" in location.lower():
            return f"The weather in {location} is 70°F and sunny."
        elif "san francisco" in location.lower():
            return f"The weather in {location} is 60°F and foggy."
        else:
            return f"Sorry, I don't have weather information for {location}."

    # --- Configuration ---
    # Make sure these environment variables are set, or pass directly:
    # export CUSTOM_LLM_API_URL="YOUR_API_ENDPOINT_HERE"
    # export CUSTOM_LLM_API_KEY="YOUR_API_KEY_HERE"

    # Check if required environment variables are set
    api_url = os.getenv("CUSTOM_LLM_API_URL")
    api_key = os.getenv("CUSTOM_LLM_API_KEY")

    if not api_url or not api_key:
        print("Error: CUSTOM_LLM_API_URL and CUSTOM_LLM_API_KEY environment variables must be set.")
        print("Skipping example usage.")
    else:
        print("--- Initializing Custom LLM ---")
        # Instantiate the custom model
        # You can override auth_header/auth_prefix if needed, e.g.:
        # custom_llm = CustomChatModelWithAuth(auth_header="X-API-Key", auth_prefix="")
        custom_llm = CustomChatModelWithAuth(
            api_url=api_url,
            api_key=api_key # Will be read from env if not passed explicitly
        )

        # --- Bind the tool ---
        print("\n--- Binding Tool ---")
        llm_with_tools = custom_llm.bind_tools([get_weather])

        # --- Invoke the LLM ---
        print("\n--- Invoking LLM with Tool ---")
        try:
            # Example prompt that should trigger the tool
            prompt = "What's the weather like in Boston?"
            messages = [HumanMessage(content=prompt)]

            # Invoke the LLM (which now knows about the tool)
            ai_response = llm_with_tools.invoke(messages)

            print("\n--- LLM Response ---")
            print(f"Type: {type(ai_response)}")
            print(f"Content: {ai_response.content}")

            if ai_response.tool_calls:
                print(f"Tool Calls:")
                for tc in ai_response.tool_calls:
                    print(f"  - ID: {tc['id']}")
                    print(f"  - Name: {tc['name']}")
                    print(f"  - Args: {tc['args']}")

                # --- Simulate Tool Execution (if needed for multi-turn) ---
                # In a real agent, you would now execute the tool and feed the result back
                # tool_results = []
                # for tc in ai_response.tool_calls:
                #     if tc['name'] == 'get_weather':
                #         tool_output = get_weather.invoke(tc['args']) # Call the actual tool func
                #         tool_results.append(ToolMessage(content=str(tool_output), tool_call_id=tc['id']))
                #
                # print("\n--- Invoking LLM with Tool Result ---")
                # messages.append(ai_response) # Add AI's message requesting tool call
                # messages.extend(tool_results) # Add tool results
                # final_response = llm_with_tools.invoke(messages)
                # print("\n--- Final LLM Response ---")
                # print(final_response.content)

            else:
                print("No tool calls were made.")

        except (ConnectionError, ValueError, NotImplementedError) as e:
            print(f"\n--- An error occurred during invocation ---")
            print(e)
        except Exception as e:
            print(f"\n--- An unexpected error occurred ---")
            print(e)

