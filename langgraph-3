# Basic Auth Credentials
APP_USER=admin
APP_PASSWORD=secret_password

# SQL Server Connection (Windows Auth - Server and DB names)
SQL_SERVER_NAME=YOUR_SQL_SERVER_INSTANCE_NAME
SQL_DATABASE_NAME=YourIncidentDetailsDB
SQL_TABLE_NAME=Incidents # Or your specific table name
# Optional: Driver name if not the default
# SQL_DRIVER='{ODBC Driver 17 for SQL Server}'

# PostgreSQL Connection
PG_HOST=localhost
PG_PORT=5432
PG_DATABASE=incident_history_db
PG_USER=your_pg_user
PG_PASSWORD=your_pg_password
PG_TABLE_NAME=incident_history

# Custom LLM API Endpoint (Example)
CUSTOM_LLM_ENDPOINT=http://localhost:5000/generate # Replace with your actual endpoint
# Add any necessary API keys if your LLM requires them
# CUSTOM_LLM_API_KEY=your_llm_api_key


#app.py

import os
import functools
import json
from typing import TypedDict, Annotated, Sequence, Optional, Dict, Any
import operator

# --- Database & Web ---
import flask
from flask import request, jsonify, Response
import pyodbc
import psycopg2
from psycopg2.extras import RealDictCursor
# from pgvector.psycopg2 import register_vector # Uncomment if using pgvector extension

# --- AI & Embeddings ---
from langchain_core.messages import BaseMessage, HumanMessage, ToolMessage
from langchain_core.tools import tool
from sentence_transformers import SentenceTransformer
import numpy as np
import requests # To call the custom LLM API

# --- LangGraph ---
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode # If using Langchain Tool format

# --- Environment Variables ---
from dotenv import load_dotenv
load_dotenv()

# --- Configuration ---
APP_USER = os.getenv("APP_USER", "admin")
APP_PASSWORD = os.getenv("APP_PASSWORD", "password")

# SQL Server Config
SQL_SERVER = os.getenv("SQL_SERVER_NAME")
SQL_DATABASE = os.getenv("SQL_DATABASE_NAME")
SQL_INCIDENT_TABLE = os.getenv("SQL_TABLE_NAME", "Incidents")
SQL_DRIVER = os.getenv("SQL_DRIVER", "{ODBC Driver 17 for SQL Server}") # Adjust driver if needed

# PostgreSQL Config
PG_HOST = os.getenv("PG_HOST")
PG_PORT = os.getenv("PG_PORT")
PG_DATABASE = os.getenv("PG_DATABASE")
PG_USER = os.getenv("PG_USER")
PG_PASSWORD = os.getenv("PG_PASSWORD")
PG_HISTORY_TABLE = os.getenv("PG_TABLE_NAME", "incident_history")

# Custom LLM Config
CUSTOM_LLM_ENDPOINT = os.getenv("CUSTOM_LLM_ENDPOINT")
# CUSTOM_LLM_API_KEY = os.getenv("CUSTOM_LLM_API_KEY") # If needed

# --- Embedding Model ---
# Load a sentence transformer model for embeddings
# Ensure this matches the model used to populate the 'embeddings' column in Postgres
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
embedding_dim = embedding_model.get_sentence_embedding_dimension()
print(f"Embedding Dimension: {embedding_dim}")

# --- Flask App Setup ---
app = flask.Flask(__name__)

# --- Basic Authentication ---
def check_auth(username, password):
    """Check if a username/password combination is valid."""
    return username == APP_USER and password == APP_PASSWORD

def authenticate():
    """Sends a 401 response that enables basic auth."""
    return Response(
        'Could not verify your access level for that URL.\n'
        'You have to login with proper credentials', 401,
        {'WWW-Authenticate': 'Basic realm="Login Required"'})

def requires_auth(f):
    @functools.wraps(f)
    def decorated(*args, **kwargs):
        auth = request.authorization
        if not auth or not check_auth(auth.username, auth.password):
            return authenticate()
        return f(*args, **kwargs)
    return decorated

# --- Custom LLM Interface ---
def call_custom_llm(prompt: str, context: Optional[str] = None) -> str:
    """
    Placeholder function to call your custom LLM.
    Replace this with the actual API call or library usage for your LLM.
    """
    print(f"\n--- Calling Custom LLM ---")
    full_prompt = prompt
    if context:
        full_prompt = f"Context:\n{context}\n\nPrompt:\n{prompt}"

    print(f"Prompt sent to LLM:\n{full_prompt}\n---")

    if not CUSTOM_LLM_ENDPOINT:
        print("WARNING: CUSTOM_LLM_ENDPOINT not set. Returning placeholder response.")
        return "Placeholder LLM response: Could not connect to LLM."

    try:
        # Example for a simple JSON API endpoint
        payload = {"prompt": full_prompt}
        headers = {"Content-Type": "application/json"}
        # Add Authorization header if needed:
        # headers["Authorization"] = f"Bearer {CUSTOM_LLM_API_KEY}"

        response = requests.post(CUSTOM_LLM_ENDPOINT, json=payload, headers=headers, timeout=60)
        response.raise_for_status() # Raise an exception for bad status codes

        # --- Adjust based on your LLM's response format ---
        # Example 1: Simple text response
        # llm_response = response.text

        # Example 2: JSON response with a specific key
        llm_response = response.json().get("generated_text", "LLM response format not recognized.")
        # ----------------------------------------------------

        print(f"LLM Response: {llm_response}")
        return llm_response

    except requests.exceptions.RequestException as e:
        print(f"Error calling Custom LLM: {e}")
        return f"Error: Could not get response from LLM. Details: {e}"
    except json.JSONDecodeError:
         print(f"Error decoding LLM JSON response. Raw response: {response.text}")
         return f"Error: Could not parse LLM response. Raw: {response.text}"

# --- Embedding Function ---
def get_embedding(text: str) -> np.ndarray:
    """Generates embedding for the given text."""
    return embedding_model.encode(text)

# --- Custom Tools ---

@tool("incident_details_tool")
def incident_details_tool(incident_id: str) -> str:
    """
    Connects to the SQL Server database using Windows Authentication
    and retrieves details for a specific incident ID.
    """
    print(f"\n--- Running Incident Details Tool ---")
    print(f"Fetching details for Incident ID: {incident_id}")
    conn_str = (
        f'DRIVER={SQL_DRIVER};'
        f'SERVER={SQL_SERVER};'
        f'DATABASE={SQL_DATABASE};'
        f'Trusted_Connection=yes;' # Key for Windows Authentication
    )
    details = {}
    try:
        with pyodbc.connect(conn_str, timeout=5) as conn:
            with conn.cursor() as cursor:
                # IMPORTANT: Adjust the query, table name, and column names
                query = f"SELECT IncidentNumber, Description, Status, Priority, ReportedTime FROM {SQL_INCIDENT_TABLE} WHERE IncidentID = ?"
                print(f"Executing SQL Query: {query} with param: {incident_id}")
                cursor.execute(query, incident_id)
                row = cursor.fetchone()
                if row:
                    # Assuming column names are as specified in the query
                    columns = [column[0] for column in cursor.description]
                    details = dict(zip(columns, row))
                    print(f"Details Found: {details}")
                    return json.dumps(details) # Return details as a JSON string
                else:
                    print("Incident ID not found.")
                    return json.dumps({"error": f"Incident ID '{incident_id}' not found in {SQL_DATABASE}."})
    except pyodbc.Error as ex:
        sqlstate = ex.args[0]
        error_message = f"SQL Server connection or query failed. SQLSTATE: {sqlstate}. Error: {ex}"
        print(f"ERROR: {error_message}")
        return json.dumps({"error": error_message})
    except Exception as e:
        error_message = f"An unexpected error occurred in incident_details_tool: {e}"
        print(f"ERROR: {error_message}")
        return json.dumps({"error": error_message})

@tool("incident_resolution_finder")
def incident_resolution_finder(incident_description: str, top_k: int = 3) -> str:
    """
    Finds similar historical incidents in PostgreSQL using vector embeddings (RAG)
    and returns their details and resolutions.
    """
    print(f"\n--- Running Incident Resolution Finder (RAG) ---")
    print(f"Finding similar incidents for description: '{incident_description[:100]}...'")
    try:
        # 1. Generate embedding for the input description
        query_embedding = get_embedding(incident_description)

        # 2. Connect to PostgreSQL
        conn_pg = psycopg2.connect(
            host=PG_HOST,
            port=PG_PORT,
            database=PG_DATABASE,
            user=PG_USER,
            password=PG_PASSWORD
        )
        # register_vector(conn_pg) # Uncomment if using pgvector extension

        cur = conn_pg.cursor(cursor_factory=RealDictCursor) # Get results as dicts

        # 3. Perform Similarity Search
        # IMPORTANT: This query assumes you are using the pgvector extension
        # and its cosine distance operator (<=>). Adjust if using a different method.
        # Ensure the 'embeddings' column is indexed for performance.
        query = f"""
            SELECT
                id,
                incident_number,
                incident_details,
                resolution_comments,
                1 - (embeddings <=> %s::vector) AS similarity
            FROM {PG_HISTORY_TABLE}
            ORDER BY similarity DESC
            LIMIT %s
        """
        params = (query_embedding.tolist(), top_k)

        # --- Alternative if NOT using pgvector (less efficient) ---
        # Fetch all embeddings and calculate similarity in Python
        # query = f"SELECT id, incident_number, incident_details, resolution_comments, embeddings FROM {PG_HISTORY_TABLE}"
        # cur.execute(query)
        # all_incidents = cur.fetchall()
        # similarities = []
        # for incident in all_incidents:
        #     # Assuming embeddings are stored as lists/arrays convertible to numpy
        #     db_embedding = np.array(incident['embeddings'])
        #     # Calculate cosine similarity
        #     sim = np.dot(query_embedding, db_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(db_embedding))
        #     similarities.append({'similarity': sim, **incident})
        # # Sort by similarity and take top_k
        # similar_incidents = sorted(similarities, key=lambda x: x['similarity'], reverse=True)[:top_k]
        # params = None # Clear params if using Python calculation
        # ------------------------------------------------------------

        print(f"Executing RAG Query on PostgreSQL...")
        if params:
            cur.execute(query, params)
            similar_incidents = cur.fetchall()
        else:
            # This branch is for the Python calculation alternative (already computed)
            pass

        cur.close()
        conn_pg.close()

        if not similar_incidents:
            print("No similar incidents found.")
            return json.dumps({"message": "No similar historical incidents found."})

        # 4. Format results
        print(f"Found {len(similar_incidents)} similar incidents.")
        results = []
        for incident in similar_incidents:
             # Ensure numpy floats are converted for JSON serialization
             similarity_score = float(incident['similarity']) if 'similarity' in incident else None
             results.append({
                 "id": incident['id'],
                 "incident_number": incident.get('incident_number', 'N/A'),
                 "incident_details": incident.get('incident_details', 'N/A'),
                 "resolution_comments": incident.get('resolution_comments', 'N/A'),
                 "similarity": f"{similarity_score:.4f}" if similarity_score is not None else "N/A"
             })

        return json.dumps(results) # Return results as JSON string

    except psycopg2.Error as e:
        error_message = f"PostgreSQL connection or query failed: {e}"
        print(f"ERROR: {error_message}")
        return json.dumps({"error": error_message})
    except ImportError:
        error_message = "pgvector not installed or failed to import. Cannot perform vector search directly in DB. Install 'pgvector'."
        print(f"ERROR: {error_message}")
        return json.dumps({"error": error_message})
    except Exception as e:
        error_message = f"An unexpected error occurred in incident_resolution_finder: {e}"
        print(f"ERROR: {error_message}")
        return json.dumps({"error": error_message})

# --- LangGraph State Definition ---

class AgentState(TypedDict):
    # Input query from the user
    input_query: str
    # Optional Incident ID extracted or provided
    incident_id: Optional[str]
    # Details fetched from SQL Server
    incident_details: Optional[Dict[str, Any]]
    # Results from RAG search
    similar_incidents: Optional[List[Dict[str, Any]]]
    # Intermediate messages or LLM calls
    messages: Annotated[Sequence[BaseMessage], operator.add]
    # Final generated response
    final_response: str

# --- LangGraph Nodes ---

def start_node(state: AgentState) -> Dict[str, Any]:
    """Initial node to prepare the input."""
    print("\n--- Starting Workflow ---")
    # Potentially extract incident ID from input_query here if needed
    # For now, we assume incident_id might be passed directly or not needed initially
    return {"messages": [HumanMessage(content=state["input_query"])]}

def fetch_incident_details_node(state: AgentState) -> Dict[str, Any]:
    """Calls the Incident Details Tool if an incident_id is present."""
    print("\n--- Node: Fetch Incident Details ---")
    incident_id = state.get("incident_id")
    if incident_id:
        print(f"Incident ID found: {incident_id}. Calling tool...")
        details_str = incident_details_tool.invoke({"incident_id": incident_id})
        try:
            details_json = json.loads(details_str)
            print(f"Tool Output (Parsed): {details_json}")
            # Create a ToolMessage to record the call and result
            tool_message = ToolMessage(content=details_str, tool_call_id="fetch_details_call") # ID is arbitrary here
            return {"incident_details": details_json, "messages": [tool_message]}
        except json.JSONDecodeError:
            print(f"Error decoding JSON from incident_details_tool: {details_str}")
            error_message = ToolMessage(content=json.dumps({"error": "Failed to parse details tool output."}), tool_call_id="fetch_details_call")
            return {"incident_details": {"error": "Failed to parse tool output."}, "messages": [error_message]}
    else:
        print("No Incident ID provided. Skipping detail fetch.")
        return {}

def find_resolutions_node(state: AgentState) -> Dict[str, Any]:
    """Calls the Incident Resolution Finder (RAG) tool."""
    print("\n--- Node: Find Similar Resolutions (RAG) ---")
    # Use the original query or fetched details as input for RAG
    details = state.get("incident_details")
    input_desc = ""
    if details and not details.get("error") and details.get("Description"):
         input_desc = details["Description"]
         print(f"Using fetched description for RAG: '{input_desc[:100]}...'")
    else:
         input_desc = state["input_query"]
         print(f"Using input query for RAG: '{input_desc[:100]}...'")

    if not input_desc:
        print("WARN: No description available for RAG search.")
        return {"similar_incidents": {"error": "No description available for search."}}

    rag_results_str = incident_resolution_finder.invoke({"incident_description": input_desc})
    try:
        rag_results_json = json.loads(rag_results_str)
        print(f"RAG Tool Output (Parsed): {rag_results_json}")
        tool_message = ToolMessage(content=rag_results_str, tool_call_id="find_resolutions_call")
        return {"similar_incidents": rag_results_json, "messages": [tool_message]}
    except json.JSONDecodeError:
        print(f"Error decoding JSON from incident_resolution_finder: {rag_results_str}")
        error_message = ToolMessage(content=json.dumps({"error": "Failed to parse RAG tool output."}), tool_call_id="find_resolutions_call")
        return {"similar_incidents": {"error": "Failed to parse tool output."}, "messages": [error_message]}

def generate_response_node(state: AgentState) -> Dict[str, Any]:
    """Generates the final response using the custom LLM."""
    print("\n--- Node: Generate Final Response ---")
    current_incident_details = state.get("incident_details")
    similar_incidents = state.get("similar_incidents")
    user_query = state["input_query"]

    # --- Build Context for the LLM ---
    context_parts = []
    context_parts.append(f"User Query: {user_query}")

    if current_incident_details and not current_incident_details.get("error"):
        context_parts.append("\nCurrent Incident Details:")
        context_parts.append(json.dumps(current_incident_details, indent=2))
    elif current_incident_details and current_incident_details.get("error"):
         context_parts.append(f"\nError fetching current incident details: {current_incident_details.get('error')}")


    if similar_incidents and isinstance(similar_incidents, list):
        context_parts.append("\nSimilar Historical Incidents Found:")
        for i, incident in enumerate(similar_incidents):
            context_parts.append(f"\n--- Suggestion {i+1} (Similarity: {incident.get('similarity', 'N/A')}) ---")
            context_parts.append(f"  Number: {incident.get('incident_number', 'N/A')}")
            context_parts.append(f"  Details: {incident.get('incident_details', 'N/A')}")
            context_parts.append(f"  Resolution: {incident.get('resolution_comments', 'N/A')}")
    elif similar_incidents and similar_incidents.get("error"):
         context_parts.append(f"\nError finding similar incidents: {similar_incidents.get('error')}")
    elif similar_incidents and similar_incidents.get("message"):
        context_parts.append(f"\nHistorical Search Result: {similar_incidents.get('message')}")


    context = "\n".join(context_parts)

    # --- Define the Prompt for the LLM ---
    prompt = (
        "You are an AI assistant helping with IT incident management.\n"
        "Based on the user's query and the provided context (current incident details, similar historical incidents), "
        "synthesize a helpful response. Summarize the situation and suggest potential resolution steps based on historical data.\n"
        "If details were fetched, mention them. If similar incidents were found, highlight the most relevant resolutions.\n"
        "If errors occurred during information retrieval, acknowledge them politely."
    )

    # Call the custom LLM
    final_response = call_custom_llm(prompt=prompt, context=context)

    return {"final_response": final_response}

# --- Build LangGraph Workflow ---
workflow = StateGraph(AgentState)

# Add nodes
workflow.add_node("start", start_node)
workflow.add_node("fetch_details", fetch_incident_details_node)
workflow.add_node("find_resolutions", find_resolutions_node)
workflow.add_node("generate_response", generate_response_node)

# Define edges
workflow.set_entry_point("start")
workflow.add_edge("start", "fetch_details") # Always try to fetch details first
workflow.add_edge("fetch_details", "find_resolutions") # Then find similar incidents
workflow.add_edge("find_resolutions", "generate_response") # Then generate the final summary
workflow.add_edge("generate_response", END) # End the workflow

# Compile the graph
incident_management_app = workflow.compile()

# --- Flask Routes ---
@app.route('/')
def index():
    # Simple welcome page (optional)
    return "Incident Management AI Assistant API"

@app.route('/process_incident', methods=['POST'])
@requires_auth # Apply basic authentication
def process_incident():
    """API endpoint to process an incident query."""
    if not request.is_json:
        return jsonify({"error": "Request must be JSON"}), 400

    data = request.get_json()
    query = data.get('query')
    incident_id = data.get('incident_id') # Optional incident ID

    if not query:
        return jsonify({"error": "Missing 'query' field in request"}), 400

    print(f"\n--- Received API Request ---")
    print(f"Query: {query}")
    print(f"Incident ID: {incident_id}")

    # Initial state for the LangGraph workflow
    initial_state = {
        "input_query": query,
        "incident_id": incident_id,
        "messages": [] # Start with empty messages
    }

    try:
        # Invoke the LangGraph application
        final_state = incident_management_app.invoke(initial_state, {"recursion_limit": 10}) # Add recursion limit

        # Return the final response
        return jsonify({
            "query": query,
            "incident_id": incident_id,
            "response": final_state.get("final_response", "No response generated."),
            # Optionally return intermediate state for debugging:
            # "debug_details_fetched": final_state.get("incident_details"),
            # "debug_similar_incidents": final_state.get("similar_incidents")
        })

    except Exception as e:
        print(f"ERROR: LangGraph invocation failed: {e}")
        # Consider logging the full traceback here
        import traceback
        traceback.print_exc()
        return jsonify({"error": f"Workflow execution failed: {e}"}), 500

# --- Run Flask App ---
if __name__ == '__main__':
    # Make sure to set debug=False in production
    app.run(debug=True, host='0.0.0.0', port=5001) # Run on port 5001, accessible externally


